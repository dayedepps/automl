{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import preprocessing\n",
      "from sklearn import linear_model\n",
      "from sklearn import datasets\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import shutil, os\n",
      "from os import path\n",
      "import sqlite3\n",
      "import json\n",
      "from sklearn.externals import joblib\n",
      "from IPython.core.display import display\n",
      "import copy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## SCRIPT:\n",
      "- setup project folder layout\n",
      "- Load iris data, persist it, add necessary *meta-data*\n",
      "- Create feuture extrators: PCA, 2nd Polynomials, Subset Sampling (row-wise), Persist them, add *meta-data*, Persist intermediate data \n",
      "- Configure pipeline (deep architecture), test if the pipeline matches \n",
      "- Run the pipeline on data - stepwise / as-a-whole?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## LOWER IO HELPER FUNCTIONS\n",
      "def insert_to_db(db, table, row):\n",
      "    \"\"\"\n",
      "    db: sqlite3 datafile path\n",
      "    tablename: table in the database\n",
      "    row: dict of {col:value}\n",
      "    \"\"\"\n",
      "    conn = sqlite3.connect(db)\n",
      "    c = conn.cursor()\n",
      "    statement = \"\"\"INSERT INTO %s (%s) VALUES (%s)\"\"\" % (table, \n",
      "                                                         ','.join(row.keys()), \n",
      "                                                         ','.join([\"'%s'\" % (s,) for s in row.values()]))\n",
      "\n",
      "    #print statement\n",
      "    c.execute(statement)\n",
      "    conn.commit()\n",
      "    conn.close()\n",
      "    \n",
      "def query_db(db, table, columns = '*', where = None):\n",
      "    \"\"\"\n",
      "    return a list of dict (with columns as KEYS, and query results as VALUES)\n",
      "    \"\"\"\n",
      "    conn = sqlite3.connect(db)\n",
      "    c = conn.cursor()\n",
      "    where = ' where ' + where if where else ''\n",
      "    statement = \"SELECT %s from %s%s\" % (','.join(columns), table, where)\n",
      "    rows = c.execute(statement)\n",
      "    column_names = columns if columns != '*' else [x[0] for x in c.description]\n",
      "    results = [dict(zip(column_names, row)) for row in rows]\n",
      "    conn.commit()\n",
      "    conn.close()\n",
      "    return results\n",
      "\n",
      "def write_item(project_path, meta, bulk, item_type):\n",
      "    \"\"\"\n",
      "    write data/model to the meta database and binary files\n",
      "    \"\"\"\n",
      "    prefix = 'data_' if item_type == 'data' else 'model_'\n",
      "    ## find the way\n",
      "    item_folder = get_config(prefix+'folder', project_path)\n",
      "    item_meta_db = get_config(prefix+'meta_db', project_path)\n",
      "    item_meta_table = get_config(prefix+'meta_table', project_path)\n",
      "    ## insert meta information to meta.db\n",
      "    insert_to_db(db = item_meta_db, table = item_meta_table, row = meta)\n",
      "    ## save data_bulk into database\n",
      "    bulk_folder = path.join(item_folder, meta['name'])\n",
      "    os.mkdir(bulk_folder)\n",
      "    bulk_file = path.join(bulk_folder, 'bulk.pkl')\n",
      "    joblib.dump(bulk, bulk_file)\n",
      "    \n",
      "def read_meta_by_name(project_path, item_name, item_type):\n",
      "    \"\"\"\n",
      "    return meta informatino of item from the corresponding meta database\n",
      "    \"\"\"\n",
      "    prefix = 'data_' if item_type == 'data' else 'model_'\n",
      "    ## find the way\n",
      "    item_folder = get_config(prefix+'folder', project_path)\n",
      "    item_meta_db = get_config(prefix+'meta_db', project_path)\n",
      "    item_meta_table = get_config(prefix+'meta_table', project_path)\n",
      "    ## query the database\n",
      "    results = query_db(item_meta_db, item_meta_table, columns='*', where = 'name = \"%s\"' % item_name)\n",
      "    return results[0] if results else None\n",
      "\n",
      "def read_bulk_by_name(project_path, item_name, item_type):\n",
      "    \"\"\"\n",
      "    load the model/data bulk into the memory and return it\n",
      "    \"\"\"\n",
      "    prefix = 'data_' if item_type == 'data' else 'model_'\n",
      "    ## find the way to bulk\n",
      "    type_folder = get_config(prefix+'folder', project_path)\n",
      "    item_file = path.join(type_folder, item_name, 'bulk.pkl')\n",
      "    item = joblib.load(item_file)\n",
      "    return item\n",
      "\n",
      "def hglue(dfs):\n",
      "    \"\"\"horizontally stack all the df in dfs\n",
      "    example: xy_df = hglue([X, y]) \n",
      "    \"\"\"\n",
      "    return pd.concat(dfs, axis = 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## TEST LOW IO\n",
      "print query_db('data/prototype_pipe/data/meta.db', 'data_meta')\n",
      "print query_db('data/prototype_pipe/data/meta.db', 'data_meta', columns=[\"name\"])\n",
      "print query_db('data/prototype_pipe/data/meta.db', 'data_meta', columns=['name', 'input_features', 'output_features'], \n",
      "               where = 'name = \"iris_original\"')\n",
      "print query_db('data/prototype_pipe/data/meta.db', 'data_meta', where = \"name='NOEXIST'\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "OperationalError",
       "evalue": "unable to open database file",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-3-a98ed8c86225>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## TEST LOW IO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mquery_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/prototype_pipe/data/meta.db'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data_meta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mquery_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/prototype_pipe/data/meta.db'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data_meta'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m print query_db('data/prototype_pipe/data/meta.db', 'data_meta', columns=['name', 'input_features', 'output_features'], \n\u001b[1;32m      5\u001b[0m                where = 'name = \"iris_original\"')\n",
        "\u001b[0;32m<ipython-input-2-c508bed479f9>\u001b[0m in \u001b[0;36mquery_db\u001b[0;34m(db, table, columns, where)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mwith\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mKEYS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mquery\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mVALUES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \"\"\"\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mwhere\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' where '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mOperationalError\u001b[0m: unable to open database file"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## COnfiguration\n",
      "\n",
      "def get_config(item, project_path = None):\n",
      "    CONFIG = {  'data_folder': path.abspath(path.join(project_path, 'data'))\n",
      "              , 'model_folder': path.abspath(path.join(project_path, 'models'))\n",
      "              , 'temp_folder': path.abspath(path.join(project_path, 'temp'))\n",
      "              , 'data_meta_db': path.abspath(path.join(project_path, 'data/meta.db'))\n",
      "              , 'data_meta_table': 'data_meta'\n",
      "              , 'data_meta_schema': \"\"\"(name text, namespace text, input_features text, \n",
      "                                      output_features text, type integer)\"\"\"\n",
      "              , 'model_meta_db': path.abspath(path.join(project_path, 'models/meta.db'))\n",
      "              , 'model_meta_table': 'model_meta'\n",
      "              , 'model_meta_schema': '(name text, type integer, train_data type)'\n",
      "              , 'data_signature_template': ('namespace', 'input_features', 'output_features', 'type')}\n",
      "    return CONFIG[item]\n",
      "\n",
      "## Type system for BOTH Data and Models, could be separated or hierarchitally organized later\n",
      "\n",
      "class DataType(object):\n",
      "    \"\"\"data type \n",
      "    PRINCIPLE: THEY MUST BE EXCLUSIVE OF EACH\n",
      "    \"\"\"\n",
      "    UNSUPERVISED = 2 ** 0\n",
      "    BINARY_CLASSIFICATION = 2 ** 1\n",
      "    MULTI_CLASSIFICATION = 2 ** 2\n",
      "    REGRESSION = 2 ** 3\n",
      "class ModelType(object):\n",
      "    \"\"\"model type\n",
      "    which type of model can handle which set of data types \n",
      "    \"\"\"\n",
      "    BINARY_CLASSIFIER = DataType.BINARY_CLASSIFICATION\n",
      "    MULTI_CLASSIFIER = DataType.BINARY_CLASSIFICATION + DataType.MULTI_CLASSIFICATION ## or relationship\n",
      "    REGRESSOR = DataType.REGRESSION\n",
      "    ## e.g. clustering, auto_encoder, or feature_selector or dim-reductor\n",
      "    FEATURE_EXTRACTOR = (DataType.UNSUPERVISED + DataType.BINARY_CLASSIFICATION \n",
      "                         + DataType.MULTI_CLASSIFICATION + DataType.REGRESSION)\n",
      "    ## e.g. missing value imputator\n",
      "    PREPROCESSOR = (DataType.UNSUPERVISED + DataType.BINARY_CLASSIFICATION \n",
      "                         + DataType.MULTI_CLASSIFICATION + DataType.REGRESSION)\n",
      "    # subsampling rows\n",
      "    SUBSAMPLER = (DataType.UNSUPERVISED + DataType.BINARY_CLASSIFICATION \n",
      "                         + DataType.MULTI_CLASSIFICATION + DataType.REGRESSION)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Data Analysis LEVEL HELPER FUNCTIONS\n",
      "\n",
      "\n",
      "\n",
      "def write_project(container_path, project_name):\n",
      "    ## check if project exists - overwrite\n",
      "    project_path = path.abspath(path.join(container_path, project_name))\n",
      "    if path.exists(project_path):\n",
      "        shutil.rmtree(project_path)\n",
      "    ## create folder for project\n",
      "    os.mkdir(project_path)\n",
      "    data_folder = get_config('data_folder', project_path)\n",
      "    model_folder = get_config('model_folder', project_path)\n",
      "    temp_folder = get_config('temp_folder', project_path)\n",
      "    ## data folder\n",
      "    os.mkdir(data_folder)\n",
      "    data_meta_db = get_config('data_meta_db', project_path)\n",
      "    data_meta_table = get_config('data_meta_table', project_path)\n",
      "    data_meta_schema = get_config('data_meta_schema', project_path)\n",
      "    conn = sqlite3.connect(data_meta_db)\n",
      "    c = conn.cursor()\n",
      "    c.execute(\"CREATE TABLE %s %s\" % (data_meta_table, data_meta_schema))\n",
      "    conn.commit()\n",
      "    conn.close()   \n",
      "    ## models folder\n",
      "    os.mkdir(model_folder)\n",
      "    model_meta_db = get_config('model_meta_db', project_path)\n",
      "    model_meta_table = get_config('model_meta_table', project_path)\n",
      "    model_meta_schema = get_config('model_meta_schema', project_path)\n",
      "    conn = sqlite3.connect(model_meta_db)\n",
      "    c = conn.cursor()\n",
      "    c.execute(\"CREATE TABLE %s %s\" % (model_meta_table, model_meta_schema))\n",
      "    conn.commit()\n",
      "    conn.close()\n",
      "    ## temp folder\n",
      "    os.mkdir(temp_folder)\n",
      "    return project_path\n",
      "    \n",
      "def write_data(project_path, data_meta, data_bulk):\n",
      "    \"\"\"\n",
      "    \"\"\"\n",
      "    write_item(project_path = project_path, meta = data_meta, bulk = data_bulk, item_type = 'data')\n",
      "    \n",
      "def write_model(project_path, model_meta, model_bulk):\n",
      "    \"\"\"\n",
      "    \"\"\"\n",
      "    write_item(project_path = project_path, meta = model_meta, bulk = model_bulk, item_type = 'model')\n",
      "    \n",
      "def read_data_meta(project_path, data_name):\n",
      "    \"\"\"\n",
      "    return dictionary of data meta, as in the data/meta.db/data_meta table\n",
      "    \"\"\"\n",
      "    meta = read_meta_by_name(project_path = project_path, item_name = data_name, item_type = 'data')\n",
      "    meta['input_features'] = json.loads(meta['input_features'])\n",
      "    meta['output_features'] = json.loads(meta['output_features'])\n",
      "    return meta\n",
      "\n",
      "def read_model_meta(project_path, model_name):\n",
      "    \"\"\"\n",
      "    \"\"\"\n",
      "    return read_meta_by_name(project_path = project_path, item_name = model_name, item_type = 'model')\n",
      "\n",
      "def read_data_bulk(project_path, data_name):\n",
      "    \"\"\"\n",
      "    return the bulk of data as what it is saved as\n",
      "    \"\"\"\n",
      "    return read_bulk_by_name(project_path, data_name, 'data')\n",
      "\n",
      "def read_model_bulk(project_path, model_name):\n",
      "    \"\"\"\n",
      "    return the bulk of data as what it is saved as\n",
      "    \"\"\"\n",
      "    return read_bulk_by_name(project_path, model_name, 'model')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## MODEL and DATA\n",
      "\n",
      "def trainable(model_meta, data_meta):\n",
      "    \"\"\"\n",
      "    to test if model trainable on certain data\n",
      "    RULE: model_type mactchs data_type\n",
      "    \"\"\"\n",
      "    ## compare type information\n",
      "    model_type, data_type = model_meta['type'], data_meta['type']\n",
      "    match = (model_type & data_type > 0)\n",
      "    return match\n",
      "\n",
      "def predictable(model_meta, data_meta):\n",
      "    \"\"\"to test if model can be used to predict on data\n",
      "    RULE: train_data's signature matches new_data's signature\n",
      "    signature of data includes (namespace, input_feats, output_feats, type)\n",
      "    \"\"\"\n",
      "    try:\n",
      "        train_meta = model_meta['train_data']\n",
      "    except:\n",
      "        raise RuntimeError('the model %s has NOT been trained on any data yet' % (model_meta['name'], ))\n",
      "    signature_template = get_config('data_signature_template')\n",
      "    train_sig = [train_meta[sig] for sig in signature_template]\n",
      "    data_sig = [data_meta[sig] for sig in signature_template]\n",
      "    ## rules to decide if data_sig is compatible with a model trained on data_sig\n",
      "    ## data.namespace == train.namespace\n",
      "    ## data.input_features >= train.input_features\n",
      "    ## data.output_features == train.output_features\n",
      "    ## data.type == train.type\n",
      "    ??\n",
      "    return compatible\n",
      "\n",
      "def train_meta_on(model_meta, data_meta, trained_model_name):\n",
      "    \"\"\"\n",
      "    create and return trained_model_meta based on the previous model meta and data meta\n",
      "    \"\"\"\n",
      "    trained_model_meta = copy.deepcopy(model_meta)\n",
      "    train_data = data_meta['name']\n",
      "    trained_model_meta.update({'name': trained_model_name, 'train_data': train_data})\n",
      "    return trained_model_meta\n",
      "    \n",
      "\n",
      "def train_on(project_path, model_name, data_name, trained_model_name):\n",
      "    \"\"\"\n",
      "    STEPS:\n",
      "    1. load model_meta and data_meta if type DOESNT match, raise Exception\n",
      "    2. load model_bulk and data_bulk into memory\n",
      "    3. call model_bulk.fit(data_bulk)\n",
      "    4. generate the newmodel and save it by trained_model_name\n",
      "    \"\"\"\n",
      "    ## test if model is trainable on data\n",
      "    model_meta = read_model_meta(project_path, model_name)\n",
      "    data_meta = read_data_meta(project_path, data_name)\n",
      "    if not trainable(model_meta, data_meta):\n",
      "        raise RuntimeError(\"model %s is not trainable on dataset %s\" % (model_name, data_name))\n",
      "    ## load into memory\n",
      "    model_bulk = read_model_bulk(project_path, model_name)\n",
      "    data_bulk = read_data_bulk(project_path, data_name)\n",
      "    ## call model.fit(data)\n",
      "    input_feats = data_meta['input_features']\n",
      "    output_feats = data_meta['output_features']\n",
      "    if len(output_feats) == 1:\n",
      "        output_feats = output_feats[0]\n",
      "    data_input = np.asarray(data_bulk.loc[:, input_feats])\n",
      "    data_output = np.asarray(data_bulk.loc[:, output_feats])\n",
      "    model_bulk.fit(data_input, data_output)\n",
      "    ## generate new model\n",
      "    trained_model_meta = train_meta_on(model_meta, data_meta, trained_model_name)\n",
      "    write_model(project_path, trained_model_meta, model_bulk)\n",
      "    \n",
      "def predict_on(project_path, model_name, data_name, predicted_data):\n",
      "    \"\"\"\n",
      "    STEP:\n",
      "    1. trace model train_data's meta\n",
      "    2. compare train_data signature with new_data signature to see if they match\n",
      "    3. signature defined as namespace/namespace1\n",
      "    \"\"\"\n",
      "    ## read meta information\n",
      "    model_meta = read_model_meta(project_path, model_name)\n",
      "    data_meta = read_data_meta(project_path, data_name)\n",
      "    if not predictable(model_meta, data_meta):\n",
      "        raise RuntimeError(\"model %s cannot predict on data %s\" % (model_name, data_name))\n",
      "    ## TODO\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## clean up all the existing folder\n",
      "project_name = 'prototype_pipe'\n",
      "container_path = 'data'\n",
      "\n",
      "project_path = write_project(container_path, project_name)\n",
      "\n",
      "!tree data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "data\r\n",
        "\u2514\u2500\u2500 prototype_pipe\r\n",
        "    \u251c\u2500\u2500 data\r\n",
        "    \u2502\u00a0\u00a0 \u2514\u2500\u2500 meta.db\r\n",
        "    \u251c\u2500\u2500 models\r\n",
        "    \u2502\u00a0\u00a0 \u2514\u2500\u2500 meta.db\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "    \u2514\u2500\u2500 temp\r\n",
        "\r\n",
        "4 directories, 2 files\r\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Data Management - create, presiste meta-data\n",
      "iris = datasets.load_iris()\n",
      "iris_meta = {  'name': 'iris_original' \n",
      "             , 'namespace': 'iris'\n",
      "             , 'input_features': json.dumps([\"SepalLength\", \"SepalWidth\", \n",
      "                                     'PetalLength', 'PetalWidth'])\n",
      "             , 'output_features': json.dumps([\"Species\"])\n",
      "             , 'type': DataType.MULTI_CLASSIFICATION}\n",
      "X = pd.DataFrame(iris.data, columns = [\"SepalLength\", \"SepalWidth\", \n",
      "                            'PetalLength', 'PetalWidth'])\n",
      "y = pd.DataFrame(iris.target, columns = ['Species'])\n",
      "iris_bulk = hglue([X, y])\n",
      "write_data(project_path, iris_meta, iris_bulk)\n",
      "\n",
      "!tree data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "data\r\n",
        "\u2514\u2500\u2500 prototype_pipe\r\n",
        "    \u251c\u2500\u2500 data\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 iris_original\r\n",
        "    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl\r\n",
        "    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_01.npy\r\n",
        "    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 bulk.pkl_02.npy\r\n",
        "    \u2502\u00a0\u00a0 \u2514\u2500\u2500 meta.db\r\n",
        "    \u251c\u2500\u2500 models\r\n",
        "    \u2502\u00a0\u00a0 \u2514\u2500\u2500 meta.db\r\n",
        "    \u2514\u2500\u2500 temp\r\n",
        "\r\n",
        "5 directories, 5 files\r\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Model management - create, persiste, meta-data\n",
      "sgd_cls = linear_model.SGDClassifier()\n",
      "sgdcls_meta = {'name': 'sgd_classifier', 'type': ModelType.MULTI_CLASSIFIER}\n",
      "write_model(project_path, sgdcls_meta, sgd_cls)\n",
      "\n",
      "## sgd regression just for testing\n",
      "sgd_reg = linear_model.SGDRegressor()\n",
      "sgdreg_meta = {'name': 'sgd_regressor', 'type': ModelType.REGRESSOR}\n",
      "write_model(project_path, sgdreg_meta, sgd_reg)\n",
      "\n",
      "print trainable(read_model_meta(project_path, 'sgd_classifier'), \n",
      "                   read_data_meta(project_path, 'iris_original'))\n",
      "print trainable(read_model_meta(project_path, 'sgd_regressor'), \n",
      "                   read_data_meta(project_path, 'iris_original'))\n",
      "\n",
      "#train_on(project_path, 'sgd_regressor', 'iris_original', 'sgd_regressor_iris')\n",
      "train_on(project_path, 'sgd_classifier', 'iris_original', 'sgd_classifier_iris')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "False\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!tree"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".\r\n",
        "\u251c\u2500\u2500 data\r\n",
        "\u2502\u00a0\u00a0 \u2514\u2500\u2500 prototype_pipe\r\n",
        "\u2502\u00a0\u00a0     \u251c\u2500\u2500 data\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 iris_original\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_01.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 bulk.pkl_02.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 meta.db\r\n",
        "\u2502\u00a0\u00a0     \u251c\u2500\u2500 models\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta.db\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 sgd_classifier\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 bulk.pkl\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 sgd_classifier_iris\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_01.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_02.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_03.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 bulk.pkl_04.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 sgd_regressor\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2514\u2500\u2500 bulk.pkl\r\n",
        "\u2502\u00a0\u00a0     \u2514\u2500\u2500 temp\r\n",
        "\u2514\u2500\u2500 iris_pipelines.ipynb\r\n",
        "\r\n",
        "9 directories, 13 files\r\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## apply model to data\n",
      "\n",
      "## TRAINING part"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Construct pipeline - configure, match steps, persist"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Rung pipeline to see the result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}