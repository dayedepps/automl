{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load pipeline.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import preprocessing\n",
      "from sklearn import linear_model\n",
      "from sklearn import datasets\n",
      "from sklearn import metrics\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import shutil, os\n",
      "from os import path\n",
      "import sqlite3\n",
      "import json\n",
      "from sklearn.externals import joblib\n",
      "from IPython.core.display import display\n",
      "import copy\n",
      "\n",
      "## LOWER IO HELPER FUNCTIONS\n",
      "def insert_to_db(db, table, row):\n",
      "    \"\"\"\n",
      "    db: sqlite3 datafile path\n",
      "    tablename: table in the database\n",
      "    row: dict of {col:value}\n",
      "    \"\"\"\n",
      "    conn = sqlite3.connect(db)\n",
      "    c = conn.cursor()\n",
      "    statement = \"\"\"INSERT INTO %s (%s) VALUES (%s)\"\"\" % (table, \n",
      "                                                         ','.join(row.keys()), \n",
      "                                                         ','.join([\"'%s'\" % (s,) for s in row.values()]))\n",
      "\n",
      "    #print statement\n",
      "    c.execute(statement)\n",
      "    conn.commit()\n",
      "    conn.close()\n",
      "    \n",
      "def query_db(db, table, columns = '*', where = None):\n",
      "    \"\"\"\n",
      "    return a list of dict (with columns as KEYS, and query results as VALUES)\n",
      "    \"\"\"\n",
      "    conn = sqlite3.connect(db)\n",
      "    c = conn.cursor()\n",
      "    where = ' where ' + where if where else ''\n",
      "    statement = \"SELECT %s from %s%s\" % (','.join(columns), table, where)\n",
      "    rows = c.execute(statement)\n",
      "    column_names = columns if columns != '*' else [x[0] for x in c.description]\n",
      "    results = [dict(zip(column_names, row)) for row in rows]\n",
      "    conn.commit()\n",
      "    conn.close()\n",
      "    return results\n",
      "\n",
      "def write_item(project_path, meta, bulk, item_type):\n",
      "    \"\"\"\n",
      "    write data/model to the meta database and binary files\n",
      "    \"\"\"\n",
      "    prefix = 'data_' if item_type == 'data' else 'model_'\n",
      "    ## find the way\n",
      "    item_folder = get_config(prefix+'folder', project_path)\n",
      "    item_meta_db = get_config(prefix+'meta_db', project_path)\n",
      "    item_meta_table = get_config(prefix+'meta_table', project_path)\n",
      "    ## insert meta information to meta.db\n",
      "    insert_to_db(db = item_meta_db, table = item_meta_table, row = meta)\n",
      "    ## save data_bulk into database\n",
      "    bulk_folder = path.join(item_folder, meta['name'])\n",
      "    os.mkdir(bulk_folder)\n",
      "    bulk_file = path.join(bulk_folder, 'bulk.pkl')\n",
      "    joblib.dump(bulk, bulk_file)\n",
      "    \n",
      "def read_meta_by_name(project_path, item_name, item_type):\n",
      "    \"\"\"\n",
      "    return meta informatino of item from the corresponding meta database\n",
      "    \"\"\"\n",
      "    prefix = 'data_' if item_type == 'data' else 'model_'\n",
      "    ## find the way\n",
      "    item_folder = get_config(prefix+'folder', project_path)\n",
      "    item_meta_db = get_config(prefix+'meta_db', project_path)\n",
      "    item_meta_table = get_config(prefix+'meta_table', project_path)\n",
      "    ## query the database\n",
      "    results = query_db(item_meta_db, item_meta_table, columns='*', where = 'name = \"%s\"' % item_name)\n",
      "    return results[0] if results else None\n",
      "\n",
      "def read_bulk_by_name(project_path, item_name, item_type):\n",
      "    \"\"\"\n",
      "    load the model/data bulk into the memory and return it\n",
      "    \"\"\"\n",
      "    prefix = 'data_' if item_type == 'data' else 'model_'\n",
      "    ## find the way to bulk\n",
      "    type_folder = get_config(prefix+'folder', project_path)\n",
      "    item_file = path.join(type_folder, item_name, 'bulk.pkl')\n",
      "    item = joblib.load(item_file)\n",
      "    return item\n",
      "\n",
      "def hglue(dfs):\n",
      "    \"\"\"horizontally stack all the df in dfs\n",
      "    example: xy_df = hglue([X, y]) \n",
      "    \"\"\"\n",
      "    return pd.concat(dfs, axis = 1)\n",
      "\n",
      "def get_config(item, project_path = None):\n",
      "    project_path = project_path or '.' ## stupid tradeoff, should be improved LATER!!\n",
      "    CONFIG = {  'data_folder': path.abspath(path.join(project_path, 'data'))\n",
      "              , 'model_folder': path.abspath(path.join(project_path, 'models'))\n",
      "              , 'temp_folder': path.abspath(path.join(project_path, 'temp'))\n",
      "              , 'data_meta_db': path.abspath(path.join(project_path, 'data/meta.db'))\n",
      "              , 'data_meta_table': 'data_meta'\n",
      "              , 'data_meta_schema': \"\"\"(name text, namespace text, input_features text, \n",
      "                                      output_features text, type integer)\"\"\"\n",
      "              , 'model_meta_db': path.abspath(path.join(project_path, 'models/meta.db'))\n",
      "              , 'model_meta_table': 'model_meta'\n",
      "              , 'model_meta_schema': '(name text, type integer, train_data type)'\n",
      "              , 'data_signature_template': ('namespace', 'input_features', 'output_features', 'type')}\n",
      "    return CONFIG[item]\n",
      "\n",
      "class DataType(object):\n",
      "    \"\"\"data type \n",
      "    PRINCIPLE: THEY MUST BE EXCLUSIVE OF EACH\n",
      "    \"\"\"\n",
      "    UNSUPERVISED = 2 ** 0\n",
      "    BINARY_CLASSIFICATION = 2 ** 1\n",
      "    MULTI_CLASSIFICATION = 2 ** 2\n",
      "    REGRESSION = 2 ** 3\n",
      "class ModelType(object):\n",
      "    \"\"\"model type\n",
      "    which type of model can handle which set of data types \n",
      "    \"\"\"\n",
      "    BINARY_CLASSIFIER = DataType.BINARY_CLASSIFICATION\n",
      "    MULTI_CLASSIFIER = DataType.BINARY_CLASSIFICATION + DataType.MULTI_CLASSIFICATION ## or relationship\n",
      "    REGRESSOR = DataType.REGRESSION\n",
      "    ## e.g. clustering, auto_encoder, or feature_selector or dim-reductor\n",
      "    FEATURE_EXTRACTOR = (DataType.UNSUPERVISED + DataType.BINARY_CLASSIFICATION \n",
      "                         + DataType.MULTI_CLASSIFICATION + DataType.REGRESSION)\n",
      "    ## e.g. missing value imputator\n",
      "    PREPROCESSOR = (DataType.UNSUPERVISED + DataType.BINARY_CLASSIFICATION \n",
      "                         + DataType.MULTI_CLASSIFICATION + DataType.REGRESSION)\n",
      "    # subsampling rows\n",
      "    SUBSAMPLER = (DataType.UNSUPERVISED + DataType.BINARY_CLASSIFICATION \n",
      "                         + DataType.MULTI_CLASSIFICATION + DataType.REGRESSION)\n",
      "\n",
      "def write_project(container_path, project_name):\n",
      "    ## check if project exists - overwrite\n",
      "    project_path = path.abspath(path.join(container_path, project_name))\n",
      "    if path.exists(project_path):\n",
      "        shutil.rmtree(project_path)\n",
      "    ## create folder for project\n",
      "    os.mkdir(project_path)\n",
      "    data_folder = get_config('data_folder', project_path)\n",
      "    model_folder = get_config('model_folder', project_path)\n",
      "    temp_folder = get_config('temp_folder', project_path)\n",
      "    ## data folder\n",
      "    os.mkdir(data_folder)\n",
      "    data_meta_db = get_config('data_meta_db', project_path)\n",
      "    data_meta_table = get_config('data_meta_table', project_path)\n",
      "    data_meta_schema = get_config('data_meta_schema', project_path)\n",
      "    conn = sqlite3.connect(data_meta_db)\n",
      "    c = conn.cursor()\n",
      "    c.execute(\"CREATE TABLE %s %s\" % (data_meta_table, data_meta_schema))\n",
      "    conn.commit()\n",
      "    conn.close()   \n",
      "    ## models folder\n",
      "    os.mkdir(model_folder)\n",
      "    model_meta_db = get_config('model_meta_db', project_path)\n",
      "    model_meta_table = get_config('model_meta_table', project_path)\n",
      "    model_meta_schema = get_config('model_meta_schema', project_path)\n",
      "    conn = sqlite3.connect(model_meta_db)\n",
      "    c = conn.cursor()\n",
      "    c.execute(\"CREATE TABLE %s %s\" % (model_meta_table, model_meta_schema))\n",
      "    conn.commit()\n",
      "    conn.close()\n",
      "    ## temp folder\n",
      "    os.mkdir(temp_folder)\n",
      "    return project_path\n",
      "    \n",
      "def write_data(project_path, data_meta, data_bulk):\n",
      "    \"\"\"\n",
      "    \"\"\"\n",
      "    meta = copy.deepcopy(data_meta)\n",
      "    meta['input_features'] = json.dumps(meta['input_features'])\n",
      "    meta['output_features'] = json.dumps(meta['output_features'])\n",
      "    write_item(project_path = project_path, meta = meta, bulk = data_bulk, item_type = 'data')\n",
      "    \n",
      "def write_model(project_path, model_meta, model_bulk):\n",
      "    \"\"\"\n",
      "    \"\"\"\n",
      "    write_item(project_path = project_path, meta = model_meta, bulk = model_bulk, item_type = 'model')\n",
      "    \n",
      "def read_data_meta(project_path, data_name):\n",
      "    \"\"\"\n",
      "    return dictionary of data meta, as in the data/meta.db/data_meta table\n",
      "    \"\"\"\n",
      "    meta = read_meta_by_name(project_path = project_path, item_name = data_name, item_type = 'data')\n",
      "    meta['input_features'] = json.loads(meta['input_features'])\n",
      "    meta['output_features'] = json.loads(meta['output_features'])\n",
      "    return meta\n",
      "\n",
      "def read_model_meta(project_path, model_name):\n",
      "    \"\"\"\n",
      "    \"\"\"\n",
      "    return read_meta_by_name(project_path = project_path, item_name = model_name, item_type = 'model')\n",
      "\n",
      "def read_data_bulk(project_path, data_name):\n",
      "    \"\"\"\n",
      "    return the bulk of data as what it is saved as\n",
      "    \"\"\"\n",
      "    return read_bulk_by_name(project_path, data_name, 'data')\n",
      "\n",
      "def read_model_bulk(project_path, model_name):\n",
      "    \"\"\"\n",
      "    return the bulk of data as what it is saved as\n",
      "    \"\"\"\n",
      "    return read_bulk_by_name(project_path, model_name, 'model')\n",
      "\n",
      "def trainable(model_meta, data_meta):\n",
      "    \"\"\"\n",
      "    to test if model trainable on certain data\n",
      "    RULE: model_type mactchs data_type\n",
      "    \"\"\"\n",
      "    ## compare type information\n",
      "    model_type, data_type = model_meta['type'], data_meta['type']\n",
      "    match = (model_type & data_type > 0)\n",
      "    return match\n",
      "\n",
      "def predictable(model_meta, data_meta, project_path):\n",
      "    \"\"\"to test if model can be used to predict on data\n",
      "    RULE: train_data's signature matches new_data's signature\n",
      "    signature of data includes (namespace, input_feats, output_feats, type)\n",
      "    \"\"\"\n",
      "    try:\n",
      "        train_meta = read_data_meta(project_path, model_meta['train_data'])\n",
      "    except Exception, e:\n",
      "        raise e\n",
      "        #raise RuntimeError('the model %s has NOT been trained on any data yet' % (model_meta['name'], ))\n",
      "    signature_template = get_config('data_signature_template')\n",
      "    train_sig = {sig:train_meta[sig] for sig in signature_template}\n",
      "    data_sig = {sig:data_meta[sig] for sig in signature_template}\n",
      "    ## rules to decide if data_sig is compatible with a model trained on data_sig\n",
      "    ## data.namespace == train.namespace\n",
      "    ## data.input_features >= train.input_features\n",
      "    ## data.output_features == train.output_features - NO NEED FOR PREDICTION AT ALL\n",
      "    ## data.type == train.type\n",
      "    namespace_match = data_sig['namespace'] == train_sig['namespace']\n",
      "    inputs_match = set(data_sig['input_features']).issuperset(set(train_sig['input_features']))\n",
      "    #outputs_match = set(data_sig['output_features']) == set(train_sig['output_features'])\n",
      "    type_match = data_sig['type'] == train_sig['type']\n",
      "    compatible = namespace_match and inputs_match and type_match\n",
      "    return compatible\n",
      "\n",
      "def train_meta_on(model_meta, data_meta, trained_model_name):\n",
      "    \"\"\"\n",
      "    create and return trained_model_meta based on the previous model meta and data meta\n",
      "    \"\"\"\n",
      "    trained_model_meta = copy.deepcopy(model_meta)\n",
      "    train_data = data_meta['name']\n",
      "    trained_model_meta.update({'name': trained_model_name, 'train_data': train_data})\n",
      "    return trained_model_meta\n",
      "    \n",
      "def train_on(project_path, model_name, data_name, trained_model_name):\n",
      "    \"\"\"\n",
      "    STEPS:\n",
      "    1. load model_meta and data_meta if type DOESNT match, raise Exception\n",
      "    2. load model_bulk and data_bulk into memory\n",
      "    3. call model_bulk.fit(data_bulk)\n",
      "    4. generate the newmodel and save it by trained_model_name\n",
      "    \"\"\"\n",
      "    ## test if model is trainable on data\n",
      "    model_meta = read_model_meta(project_path, model_name)\n",
      "    data_meta = read_data_meta(project_path, data_name)\n",
      "    if not trainable(model_meta, data_meta):\n",
      "        raise RuntimeError(\"model %s is not trainable on dataset %s\" % (model_name, data_name))\n",
      "    ## load into memory\n",
      "    model_bulk = read_model_bulk(project_path, model_name)\n",
      "    data_bulk = read_data_bulk(project_path, data_name)\n",
      "    ## call model.fit(data)\n",
      "    input_feats = data_meta['input_features']\n",
      "    output_feats = data_meta['output_features']\n",
      "    if len(output_feats) == 1:\n",
      "        output_feats = output_feats[0]\n",
      "    data_input = np.asarray(data_bulk.loc[:, input_feats])\n",
      "    data_output = np.asarray(data_bulk.loc[:, output_feats])\n",
      "    model_bulk.fit(data_input, data_output)\n",
      "    ## generate new model\n",
      "    trained_model_meta = train_meta_on(model_meta, data_meta, trained_model_name)\n",
      "    write_model(project_path, trained_model_meta, model_bulk)\n",
      "    \n",
      "def predict_on(project_path, model_name, data_name, predicted_data_name):\n",
      "    \"\"\"\n",
      "    STEP:\n",
      "    1. trace model train_data's meta\n",
      "    2. compare train_data signature with new_data signature to see if they match\n",
      "    3. signature defined as namespace/namespace1\n",
      "    \"\"\"\n",
      "    ## read meta information\n",
      "    model_meta = read_model_meta(project_path, model_name)\n",
      "    model_train_meta = read_data_meta(project_path, model_meta['train_data'])\n",
      "    data_meta = read_data_meta(project_path, data_name)\n",
      "    if not predictable(model_meta, data_meta, project_path):\n",
      "        raise RuntimeError(\"model %s cannot predict on data %s\" % (model_name, data_name))\n",
      "    ## load the data and trained model into memory\n",
      "    model_bulk = read_model_bulk(project_path, model_name)\n",
      "    data_bulk = read_data_bulk(project_path, data_name)\n",
      "    input_features = model_train_meta['input_features']\n",
      "    output_features = model_train_meta['output_features']\n",
      "    ## fit the data into the shape of model's train data\n",
      "    X = np.asarray(data_bulk.loc[:, input_features])\n",
      "    yhat = model_bulk.predict(X)\n",
      "    ## combine yhat with original data\n",
      "    yhat = pd.DataFrame(yhat, columns = output_features)\n",
      "    predicted_data_bulk = data_bulk\n",
      "    predicted_data_bulk.update(yhat, join = 'left')\n",
      "    predicted_data_meta = data_meta\n",
      "    predicted_data_meta.update({'name': predicted_data_name, 'output_features': output_features})\n",
      "    write_data(project_path, predicted_data_meta, predicted_data_bulk)\n",
      "\n",
      "def score_on(project_path, target_data_name, predicted_data_name, score_fn):\n",
      "    \"\"\"\n",
      "    the target_data and predicted_data should have the common set of output feature\n",
      "    the method will compare the output features and output and apply score_fn on them\n",
      "    examples of score_fn include: (1) sklearn.metrics.XX\n",
      "    TODO: consider the type of data to test if the certain score_fn is appliable to them\n",
      "    \"\"\"\n",
      "    ## read meta\n",
      "    target_meta = read_data_meta(project_path, target_data_name)\n",
      "    predicted_meta = read_data_meta(project_path, predicted_data_name)\n",
      "    assert target_meta['output_features'] == predicted_meta['output_features'] \n",
      "    output_features = target_meta['output_features']\n",
      "    ## read bulk\n",
      "    y = np.asarray(read_data_bulk(project_path, target_data_name).loc[:, output_features])\n",
      "    yhat = np.asarray(read_data_bulk(project_path, predicted_data_name).loc[:, output_features])\n",
      "    ## apply scorefn \n",
      "    return score_fn(y, yhat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import preprocessing\n",
      "from sklearn import linear_model\n",
      "from sklearn import datasets\n",
      "from sklearn import metrics\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import shutil, os\n",
      "from os import path\n",
      "import sqlite3\n",
      "import json\n",
      "from sklearn.externals import joblib\n",
      "from IPython.core.display import display\n",
      "import copy\n",
      "\n",
      "## LOWER IO HELPER FUNCTIONS\n",
      "def insert_to_db(db, table, row):\n",
      "    \"\"\"\n",
      "    db: sqlite3 datafile path\n",
      "    tablename: table in the database\n",
      "    row: dict of {col:value}\n",
      "    \"\"\"\n",
      "    conn = sqlite3.connect(db)\n",
      "    c = conn.cursor()\n",
      "    statement = \"\"\"INSERT INTO %s (%s) VALUES (%s)\"\"\" % (table, \n",
      "                                                         ','.join(row.keys()), \n",
      "                                                         ','.join([\"'%s'\" % (s,) for s in row.values()]))\n",
      "\n",
      "    #print statement\n",
      "    c.execute(statement)\n",
      "    conn.commit()\n",
      "    conn.close()\n",
      "    \n",
      "def query_db(db, table, columns = '*', where = None):\n",
      "    \"\"\"\n",
      "    return a list of dict (with columns as KEYS, and query results as VALUES)\n",
      "    \"\"\"\n",
      "    conn = sqlite3.connect(db)\n",
      "    c = conn.cursor()\n",
      "    where = ' where ' + where if where else ''\n",
      "    statement = \"SELECT %s from %s%s\" % (','.join(columns), table, where)\n",
      "    rows = c.execute(statement)\n",
      "    column_names = columns if columns != '*' else [x[0] for x in c.description]\n",
      "    results = [dict(zip(column_names, row)) for row in rows]\n",
      "    conn.commit()\n",
      "    conn.close()\n",
      "    return results\n",
      "\n",
      "def write_item(project_path, meta, bulk, item_type):\n",
      "    \"\"\"\n",
      "    write data/model to the meta database and binary files\n",
      "    \"\"\"\n",
      "    prefix = 'data_' if item_type == 'data' else 'model_'\n",
      "    ## find the way\n",
      "    item_folder = get_config(prefix+'folder', project_path)\n",
      "    item_meta_db = get_config(prefix+'meta_db', project_path)\n",
      "    item_meta_table = get_config(prefix+'meta_table', project_path)\n",
      "    ## insert meta information to meta.db\n",
      "    insert_to_db(db = item_meta_db, table = item_meta_table, row = meta)\n",
      "    ## save data_bulk into database\n",
      "    bulk_folder = path.join(item_folder, meta['name'])\n",
      "    os.mkdir(bulk_folder)\n",
      "    bulk_file = path.join(bulk_folder, 'bulk.pkl')\n",
      "    joblib.dump(bulk, bulk_file)\n",
      "    \n",
      "def read_meta_by_name(project_path, item_name, item_type):\n",
      "    \"\"\"\n",
      "    return meta informatino of item from the corresponding meta database\n",
      "    \"\"\"\n",
      "    prefix = 'data_' if item_type == 'data' else 'model_'\n",
      "    ## find the way\n",
      "    item_folder = get_config(prefix+'folder', project_path)\n",
      "    item_meta_db = get_config(prefix+'meta_db', project_path)\n",
      "    item_meta_table = get_config(prefix+'meta_table', project_path)\n",
      "    ## query the database\n",
      "    results = query_db(item_meta_db, item_meta_table, columns='*', where = 'name = \"%s\"' % item_name)\n",
      "    return results[0] if results else None\n",
      "\n",
      "def read_bulk_by_name(project_path, item_name, item_type):\n",
      "    \"\"\"\n",
      "    load the model/data bulk into the memory and return it\n",
      "    \"\"\"\n",
      "    prefix = 'data_' if item_type == 'data' else 'model_'\n",
      "    ## find the way to bulk\n",
      "    type_folder = get_config(prefix+'folder', project_path)\n",
      "    item_file = path.join(type_folder, item_name, 'bulk.pkl')\n",
      "    item = joblib.load(item_file)\n",
      "    return item\n",
      "\n",
      "def hglue(dfs):\n",
      "    \"\"\"horizontally stack all the df in dfs\n",
      "    example: xy_df = hglue([X, y]) \n",
      "    \"\"\"\n",
      "    return pd.concat(dfs, axis = 1)\n",
      "\n",
      "def get_config(item, project_path = None):\n",
      "    project_path = project_path or '.' ## stupid tradeoff, should be improved LATER!!\n",
      "    CONFIG = {  'data_folder': path.abspath(path.join(project_path, 'data'))\n",
      "              , 'model_folder': path.abspath(path.join(project_path, 'models'))\n",
      "              , 'temp_folder': path.abspath(path.join(project_path, 'temp'))\n",
      "              , 'data_meta_db': path.abspath(path.join(project_path, 'data/meta.db'))\n",
      "              , 'data_meta_table': 'data_meta'\n",
      "              , 'data_meta_schema': \"\"\"(name text, namespace text, input_features text, \n",
      "                                      output_features text, type integer)\"\"\"\n",
      "              , 'model_meta_db': path.abspath(path.join(project_path, 'models/meta.db'))\n",
      "              , 'model_meta_table': 'model_meta'\n",
      "              , 'model_meta_schema': '(name text, type integer, train_data type)'\n",
      "              , 'data_signature_template': ('namespace', 'input_features', 'output_features', 'type')}\n",
      "    return CONFIG[item]\n",
      "\n",
      "class DataType(object):\n",
      "    \"\"\"data type \n",
      "    PRINCIPLE: THEY MUST BE EXCLUSIVE OF EACH\n",
      "    \"\"\"\n",
      "    UNSUPERVISED = 2 ** 0\n",
      "    BINARY_CLASSIFICATION = 2 ** 1\n",
      "    MULTI_CLASSIFICATION = 2 ** 2\n",
      "    REGRESSION = 2 ** 3\n",
      "class ModelType(object):\n",
      "    \"\"\"model type\n",
      "    which type of model can handle which set of data types \n",
      "    \"\"\"\n",
      "    BINARY_CLASSIFIER = DataType.BINARY_CLASSIFICATION\n",
      "    MULTI_CLASSIFIER = DataType.BINARY_CLASSIFICATION + DataType.MULTI_CLASSIFICATION ## or relationship\n",
      "    REGRESSOR = DataType.REGRESSION\n",
      "    ## e.g. clustering, auto_encoder, or feature_selector or dim-reductor\n",
      "    FEATURE_EXTRACTOR = (DataType.UNSUPERVISED + DataType.BINARY_CLASSIFICATION \n",
      "                         + DataType.MULTI_CLASSIFICATION + DataType.REGRESSION)\n",
      "    ## e.g. missing value imputator\n",
      "    PREPROCESSOR = (DataType.UNSUPERVISED + DataType.BINARY_CLASSIFICATION \n",
      "                         + DataType.MULTI_CLASSIFICATION + DataType.REGRESSION)\n",
      "    # subsampling rows\n",
      "    SUBSAMPLER = (DataType.UNSUPERVISED + DataType.BINARY_CLASSIFICATION \n",
      "                         + DataType.MULTI_CLASSIFICATION + DataType.REGRESSION)\n",
      "\n",
      "def write_project(container_path, project_name):\n",
      "    ## check if project exists - overwrite\n",
      "    project_path = path.abspath(path.join(container_path, project_name))\n",
      "    if path.exists(project_path):\n",
      "        shutil.rmtree(project_path)\n",
      "    ## create folder for project\n",
      "    os.mkdir(project_path)\n",
      "    data_folder = get_config('data_folder', project_path)\n",
      "    model_folder = get_config('model_folder', project_path)\n",
      "    temp_folder = get_config('temp_folder', project_path)\n",
      "    ## data folder\n",
      "    os.mkdir(data_folder)\n",
      "    data_meta_db = get_config('data_meta_db', project_path)\n",
      "    data_meta_table = get_config('data_meta_table', project_path)\n",
      "    data_meta_schema = get_config('data_meta_schema', project_path)\n",
      "    conn = sqlite3.connect(data_meta_db)\n",
      "    c = conn.cursor()\n",
      "    c.execute(\"CREATE TABLE %s %s\" % (data_meta_table, data_meta_schema))\n",
      "    conn.commit()\n",
      "    conn.close()   \n",
      "    ## models folder\n",
      "    os.mkdir(model_folder)\n",
      "    model_meta_db = get_config('model_meta_db', project_path)\n",
      "    model_meta_table = get_config('model_meta_table', project_path)\n",
      "    model_meta_schema = get_config('model_meta_schema', project_path)\n",
      "    conn = sqlite3.connect(model_meta_db)\n",
      "    c = conn.cursor()\n",
      "    c.execute(\"CREATE TABLE %s %s\" % (model_meta_table, model_meta_schema))\n",
      "    conn.commit()\n",
      "    conn.close()\n",
      "    ## temp folder\n",
      "    os.mkdir(temp_folder)\n",
      "    return project_path\n",
      "    \n",
      "def write_data(project_path, data_meta, data_bulk):\n",
      "    \"\"\"\n",
      "    \"\"\"\n",
      "    meta = copy.deepcopy(data_meta)\n",
      "    meta['input_features'] = json.dumps(meta['input_features'])\n",
      "    meta['output_features'] = json.dumps(meta['output_features'])\n",
      "    write_item(project_path = project_path, meta = meta, bulk = data_bulk, item_type = 'data')\n",
      "    \n",
      "def write_model(project_path, model_meta, model_bulk):\n",
      "    \"\"\"\n",
      "    \"\"\"\n",
      "    write_item(project_path = project_path, meta = model_meta, bulk = model_bulk, item_type = 'model')\n",
      "    \n",
      "def read_data_meta(project_path, data_name):\n",
      "    \"\"\"\n",
      "    return dictionary of data meta, as in the data/meta.db/data_meta table\n",
      "    \"\"\"\n",
      "    meta = read_meta_by_name(project_path = project_path, item_name = data_name, item_type = 'data')\n",
      "    meta['input_features'] = json.loads(meta['input_features'])\n",
      "    meta['output_features'] = json.loads(meta['output_features'])\n",
      "    return meta\n",
      "\n",
      "def read_model_meta(project_path, model_name):\n",
      "    \"\"\"\n",
      "    \"\"\"\n",
      "    return read_meta_by_name(project_path = project_path, item_name = model_name, item_type = 'model')\n",
      "\n",
      "def read_data_bulk(project_path, data_name):\n",
      "    \"\"\"\n",
      "    return the bulk of data as what it is saved as\n",
      "    \"\"\"\n",
      "    return read_bulk_by_name(project_path, data_name, 'data')\n",
      "\n",
      "def read_model_bulk(project_path, model_name):\n",
      "    \"\"\"\n",
      "    return the bulk of data as what it is saved as\n",
      "    \"\"\"\n",
      "    return read_bulk_by_name(project_path, model_name, 'model')\n",
      "\n",
      "def trainable(model_meta, data_meta):\n",
      "    \"\"\"\n",
      "    to test if model trainable on certain data\n",
      "    RULE: model_type mactchs data_type\n",
      "    \"\"\"\n",
      "    ## compare type information\n",
      "    model_type, data_type = model_meta['type'], data_meta['type']\n",
      "    match = (model_type & data_type > 0)\n",
      "    return match\n",
      "\n",
      "def predictable(model_meta, data_meta, project_path):\n",
      "    \"\"\"to test if model can be used to predict on data\n",
      "    RULE: train_data's signature matches new_data's signature\n",
      "    signature of data includes (namespace, input_feats, output_feats, type)\n",
      "    \"\"\"\n",
      "    try:\n",
      "        train_meta = read_data_meta(project_path, model_meta['train_data'])\n",
      "    except Exception, e:\n",
      "        raise e\n",
      "        #raise RuntimeError('the model %s has NOT been trained on any data yet' % (model_meta['name'], ))\n",
      "    signature_template = get_config('data_signature_template')\n",
      "    train_sig = {sig:train_meta[sig] for sig in signature_template}\n",
      "    data_sig = {sig:data_meta[sig] for sig in signature_template}\n",
      "    ## rules to decide if data_sig is compatible with a model trained on data_sig\n",
      "    ## data.namespace == train.namespace\n",
      "    ## data.input_features >= train.input_features\n",
      "    ## data.output_features == train.output_features - NO NEED FOR PREDICTION AT ALL\n",
      "    ## data.type == train.type\n",
      "    namespace_match = data_sig['namespace'] == train_sig['namespace']\n",
      "    inputs_match = set(data_sig['input_features']).issuperset(set(train_sig['input_features']))\n",
      "    #outputs_match = set(data_sig['output_features']) == set(train_sig['output_features'])\n",
      "    type_match = data_sig['type'] == train_sig['type']\n",
      "    compatible = namespace_match and inputs_match and type_match\n",
      "    return compatible\n",
      "\n",
      "def train_meta_on(model_meta, data_meta, trained_model_name):\n",
      "    \"\"\"\n",
      "    create and return trained_model_meta based on the previous model meta and data meta\n",
      "    \"\"\"\n",
      "    trained_model_meta = copy.deepcopy(model_meta)\n",
      "    train_data = data_meta['name']\n",
      "    trained_model_meta.update({'name': trained_model_name, 'train_data': train_data})\n",
      "    return trained_model_meta\n",
      "    \n",
      "def train_on(project_path, model_name, data_name, trained_model_name):\n",
      "    \"\"\"\n",
      "    STEPS:\n",
      "    1. load model_meta and data_meta if type DOESNT match, raise Exception\n",
      "    2. load model_bulk and data_bulk into memory\n",
      "    3. call model_bulk.fit(data_bulk)\n",
      "    4. generate the newmodel and save it by trained_model_name\n",
      "    \"\"\"\n",
      "    ## test if model is trainable on data\n",
      "    model_meta = read_model_meta(project_path, model_name)\n",
      "    data_meta = read_data_meta(project_path, data_name)\n",
      "    if not trainable(model_meta, data_meta):\n",
      "        raise RuntimeError(\"model %s is not trainable on dataset %s\" % (model_name, data_name))\n",
      "    ## load into memory\n",
      "    model_bulk = read_model_bulk(project_path, model_name)\n",
      "    data_bulk = read_data_bulk(project_path, data_name)\n",
      "    ## call model.fit(data)\n",
      "    input_feats = data_meta['input_features']\n",
      "    output_feats = data_meta['output_features']\n",
      "    if len(output_feats) == 1:\n",
      "        output_feats = output_feats[0]\n",
      "    data_input = np.asarray(data_bulk.loc[:, input_feats])\n",
      "    data_output = np.asarray(data_bulk.loc[:, output_feats])\n",
      "    model_bulk.fit(data_input, data_output)\n",
      "    ## generate new model\n",
      "    trained_model_meta = train_meta_on(model_meta, data_meta, trained_model_name)\n",
      "    write_model(project_path, trained_model_meta, model_bulk)\n",
      "    \n",
      "def predict_on(project_path, model_name, data_name, predicted_data_name):\n",
      "    \"\"\"\n",
      "    STEP:\n",
      "    1. trace model train_data's meta\n",
      "    2. compare train_data signature with new_data signature to see if they match\n",
      "    3. signature defined as namespace/namespace1\n",
      "    \"\"\"\n",
      "    ## read meta information\n",
      "    model_meta = read_model_meta(project_path, model_name)\n",
      "    model_train_meta = read_data_meta(project_path, model_meta['train_data'])\n",
      "    data_meta = read_data_meta(project_path, data_name)\n",
      "    if not predictable(model_meta, data_meta, project_path):\n",
      "        raise RuntimeError(\"model %s cannot predict on data %s\" % (model_name, data_name))\n",
      "    ## load the data and trained model into memory\n",
      "    model_bulk = read_model_bulk(project_path, model_name)\n",
      "    data_bulk = read_data_bulk(project_path, data_name)\n",
      "    input_features = model_train_meta['input_features']\n",
      "    output_features = model_train_meta['output_features']\n",
      "    ## fit the data into the shape of model's train data\n",
      "    X = np.asarray(data_bulk.loc[:, input_features])\n",
      "    yhat = model_bulk.predict(X)\n",
      "    ## combine yhat with original data\n",
      "    yhat = pd.DataFrame(yhat, columns = output_features)\n",
      "    predicted_data_bulk = data_bulk\n",
      "    predicted_data_bulk.update(yhat, join = 'left')\n",
      "    predicted_data_meta = data_meta\n",
      "    predicted_data_meta.update({'name': predicted_data_name, 'output_features': output_features})\n",
      "    write_data(project_path, predicted_data_meta, predicted_data_bulk)\n",
      "\n",
      "def score_on(project_path, target_data_name, predicted_data_name, score_fn):\n",
      "    \"\"\"\n",
      "    the target_data and predicted_data should have the common set of output feature\n",
      "    the method will compare the output features and output and apply score_fn on them\n",
      "    examples of score_fn include: (1) sklearn.metrics.XX\n",
      "    TODO: consider the type of data to test if the certain score_fn is appliable to them\n",
      "    \"\"\"\n",
      "    ## read meta\n",
      "    target_meta = read_data_meta(project_path, target_data_name)\n",
      "    predicted_meta = read_data_meta(project_path, predicted_data_name)\n",
      "    assert target_meta['output_features'] == predicted_meta['output_features'] \n",
      "    output_features = target_meta['output_features']\n",
      "    ## read bulk\n",
      "    y = np.asarray(read_data_bulk(project_path, target_data_name).loc[:, output_features])\n",
      "    yhat = np.asarray(read_data_bulk(project_path, predicted_data_name).loc[:, output_features])\n",
      "    ## apply scorefn \n",
      "    return score_fn(y, yhat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Testing Code"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## clean up all the existing folder\n",
      "project_name = 'prototype_pipe'\n",
      "container_path = 'data'\n",
      "\n",
      "project_path = write_project(container_path, project_name)\n",
      "\n",
      "!tree data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b[01;34mdata\u001b[00m\r\n",
        "\u2514\u2500\u2500 \u001b[01;34mprototype_pipe\u001b[00m\r\n",
        "    \u251c\u2500\u2500 \u001b[01;34mdata\u001b[00m\r\n",
        "    \u2502\u00a0\u00a0 \u2514\u2500\u2500 meta.db\r\n",
        "    \u251c\u2500\u2500 \u001b[01;34mmodels\u001b[00m\r\n",
        "    \u2502\u00a0\u00a0 \u2514\u2500\u2500 meta.db\r\n",
        "    \u2514\u2500\u2500 \u001b[01;34mtemp\u001b[00m\r\n",
        "\r\n",
        "4 directories, 2 files\r\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Data Management - create, presiste meta-data\n",
      "iris = datasets.load_iris()\n",
      "iris_meta = {  'name': 'iris_original' \n",
      "             , 'namespace': 'iris'\n",
      "             , 'input_features': [\"SepalLength\", \"SepalWidth\", \n",
      "                                     'PetalLength', 'PetalWidth']\n",
      "             , 'output_features': [\"Species\"]\n",
      "             , 'type': DataType.MULTI_CLASSIFICATION}\n",
      "X = pd.DataFrame(iris.data, columns = [\"SepalLength\", \"SepalWidth\", \n",
      "                            'PetalLength', 'PetalWidth'])\n",
      "y = pd.DataFrame(iris.target, columns = ['Species'])\n",
      "iris_bulk = hglue([X, y])\n",
      "write_data(project_path, iris_meta, iris_bulk)\n",
      "\n",
      "!tree data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b[01;34mdata\u001b[00m\r\n",
        "\u2514\u2500\u2500 \u001b[01;34mprototype_pipe\u001b[00m\r\n",
        "    \u251c\u2500\u2500 \u001b[01;34mdata\u001b[00m\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 \u001b[01;34miris_original\u001b[00m\r\n",
        "    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl\r\n",
        "    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_01.npy\r\n",
        "    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 bulk.pkl_02.npy\r\n",
        "    \u2502\u00a0\u00a0 \u2514\u2500\u2500 meta.db\r\n",
        "    \u251c\u2500\u2500 \u001b[01;34mmodels\u001b[00m\r\n",
        "    \u2502\u00a0\u00a0 \u2514\u2500\u2500 meta.db\r\n",
        "    \u2514\u2500\u2500 \u001b[01;34mtemp\u001b[00m\r\n",
        "\r\n",
        "5 directories, 5 files\r\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Model management - create, persiste, meta-data\n",
      "sgd_cls = linear_model.SGDClassifier()\n",
      "sgdcls_meta = {'name': 'sgd_classifier', 'type': ModelType.MULTI_CLASSIFIER}\n",
      "write_model(project_path, sgdcls_meta, sgd_cls)\n",
      "\n",
      "## sgd regression just for testing\n",
      "sgd_reg = linear_model.SGDRegressor()\n",
      "sgdreg_meta = {'name': 'sgd_regressor', 'type': ModelType.REGRESSOR}\n",
      "write_model(project_path, sgdreg_meta, sgd_reg)\n",
      "\n",
      "print trainable(read_model_meta(project_path, 'sgd_classifier'), \n",
      "                   read_data_meta(project_path, 'iris_original'))\n",
      "print trainable(read_model_meta(project_path, 'sgd_regressor'), \n",
      "                   read_data_meta(project_path, 'iris_original'))\n",
      "\n",
      "#train_on(project_path, 'sgd_regressor', 'iris_original', 'sgd_regressor_iris')\n",
      "train_on(project_path, 'sgd_classifier', 'iris_original', 'sgd_classifier_iris')\n",
      "\n",
      "##train random forest model\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "rf_model = RandomForestClassifier(n_estimators=3, max_depth=2)\n",
      "rf_meta = {'name':'rf_classifier', 'type': ModelType.MULTI_CLASSIFIER}\n",
      "##create project folder\n",
      "write_model(project_path, rf_meta, rf_model)\n",
      "print trainable(read_model_meta(project_path, 'rf_classifier'), read_data_meta(project_path, 'iris_original'))\n",
      "train_on(project_path, 'rf_classifier', 'iris_original', 'rf_classifier_iris')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "False\n",
        "True\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## TEST LOW IO\n",
      "print query_db('data/prototype_pipe/data/meta.db', 'data_meta')\n",
      "print query_db('data/prototype_pipe/data/meta.db', 'data_meta', columns=[\"name\"])\n",
      "print query_db('data/prototype_pipe/data/meta.db', 'data_meta', columns=['name', 'input_features', 'output_features'], \n",
      "               where = 'name = \"iris_original\"')\n",
      "print query_db('data/prototype_pipe/data/meta.db', 'data_meta', where = \"name='NOEXIST'\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[{'input_features': u'[\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\"]', 'type': 4, 'namespace': u'iris', 'name': u'iris_original', 'output_features': u'[\"Species\"]'}]\n",
        "[{'name': u'iris_original'}]\n",
        "[{'output_features': u'[\"Species\"]', 'input_features': u'[\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\"]', 'name': u'iris_original'}]\n",
        "[]\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## apply model to data\n",
      "print predictable(read_model_meta(project_path, 'sgd_classifier_iris'), \n",
      "            read_data_meta(project_path, 'iris_original'), project_path)\n",
      "predict_on(project_path, 'sgd_classifier_iris', 'iris_original', 'sgd_classifier_iris_prediction')\n",
      "## make prediction on the trained model\n",
      "## apply model to data\n",
      "print predictable(read_model_meta(project_path, 'rf_classifier_iris'), \n",
      "            read_data_meta(project_path, 'iris_original'), project_path)\n",
      "predict_on(project_path, 'rf_classifier_iris', 'iris_original', 'rf_classifier_iris_prediction')\n",
      "!tree"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "True\n",
        "\u001b[01;34m.\u001b[00m\r\n",
        "\u251c\u2500\u2500 \u001b[01;34mdata\u001b[00m\r\n",
        "\u2502\u00a0\u00a0 \u2514\u2500\u2500 \u001b[01;34mprototype_pipe\u001b[00m\r\n",
        "\u2502\u00a0\u00a0     \u251c\u2500\u2500 \u001b[01;34mdata\u001b[00m\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 \u001b[01;34miris_original\u001b[00m\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_01.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 bulk.pkl_02.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta.db\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 \u001b[01;34mrf_classifier_iris_prediction\u001b[00m\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_01.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 bulk.pkl_02.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 \u001b[01;34msgd_classifier_iris_prediction\u001b[00m\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u251c\u2500\u2500 bulk.pkl\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u251c\u2500\u2500 bulk.pkl_01.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2514\u2500\u2500 bulk.pkl_02.npy\r\n",
        "\u2502\u00a0\u00a0     \u251c\u2500\u2500 \u001b[01;34mmodels\u001b[00m\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta.db\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 \u001b[01;34mrf_classifier\u001b[00m\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 bulk.pkl\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 \u001b[01;34mrf_classifier_iris\u001b[00m\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_01.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_02.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_03.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_04.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_05.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_06.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_07.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_08.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_09.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_10.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_11.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_12.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_13.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_14.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_15.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_16.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_17.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_18.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_19.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_20.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_21.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_22.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_23.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_24.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_25.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_26.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_27.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_28.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_29.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_30.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_31.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_32.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_33.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_34.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_35.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_36.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 bulk.pkl_37.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 \u001b[01;34msgd_classifier\u001b[00m\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 bulk.pkl\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 \u001b[01;34msgd_classifier_iris\u001b[00m\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_01.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_02.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bulk.pkl_03.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 bulk.pkl_04.npy\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 \u001b[01;34msgd_regressor\u001b[00m\r\n",
        "\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2514\u2500\u2500 bulk.pkl\r\n",
        "\u2502\u00a0\u00a0     \u2514\u2500\u2500 \u001b[01;34mtemp\u001b[00m\r\n",
        "\u251c\u2500\u2500 iris_pipelines.ipynb\r\n",
        "\u251c\u2500\u2500 pipeline.py\r\n",
        "\u2514\u2500\u2500 pipeline_v1.ipynb\r\n",
        "\r\n",
        "13 directories, 60 files\r\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print score_on(project_path, 'iris_original', 'sgd_classifier_iris_prediction', metrics.accuracy_score)\n",
      "print score_on(project_path, 'iris_original', 'sgd_classifier_iris_prediction', metrics.confusion_matrix)\n",
      "## test the score of the rf model\n",
      "print score_on(project_path, 'iris_original', 'rf_classifier_iris_prediction', metrics.accuracy_score)\n",
      "print score_on(project_path, 'iris_original', 'rf_classifier_iris_prediction', metrics.confusion_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.666666666667\n",
        "[[50  0  0]\n",
        " [ 0  0 50]\n",
        " [ 0  0 50]]\n",
        "0.953333333333\n",
        "[[50  0  0]\n",
        " [ 0 44  6]\n",
        " [ 0  1 49]]\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}